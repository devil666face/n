x-ollama: &ollama
  image: ollama/ollama:latest
  container_name: ollama
  restart: unless-stopped
  expose:
    - 11434/tcp
  environment:
    - OLLAMA_CONTEXT_LENGTH=8192
    - OLLAMA_FLASH_ATTENTION=1
    - OLLAMA_KV_CACHE_TYPE=q8_0
    - OLLAMA_MAX_LOADED_MODELS=2
  volumes:
    - ./data/ollama:/root/.ollama
  networks:
    - n8n
  ports:
    - 11434:11434/tcp

services:
  postgres:
    container_name: postgres
    image: postgres:13
    restart: unless-stopped
    environment:
      - POSTGRES_DB=n8n
      - POSTGRES_USER=n8n
      - POSTGRES_PASSWORD=Qwerty123
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - n8n

  n8n:
    container_name: n8n
    image: n8nio/n8n:latest
    build:
      context: n8n
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - 5678:5678/tcp
    environment:
      - N8N_HOST=n8n.local.lan
      - N8N_PORT=5678
      - N8N_PROTOCOL=https
      - NODE_ENV=production
      - WEBHOOK_URL=https://n8n.local.lan/
      - GENERIC_TIMEZONE=Europe/Moscow
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=Qwerty123
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_ENCRYPTION_KEY
      # - N8N_ENCRYPTION_KEY=a60eea2398732dcf0ad7383c688a1d3d
      - N8N_USER_MANAGEMENT_JWT_SECRET
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_USER=n8n
      - DB_POSTGRESDB_PASSWORD=Qwerty123
    volumes:
      - ./data/n8n:/root/.n8n
      - ./data/local:/files
    depends_on:
      - postgres
    networks:
      - n8n

  ollama:
    profiles: ["cpu"]
    <<: *ollama

  ollama-nvidia:
    profiles: ["nvidia"]
    <<: *ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-amd:
    profiles: ["amd"]
    <<: *ollama
    image: ollama/ollama:rocm
    devices:
      - "/dev/kfd"
      - "/dev/dri"

  # https://github.com/mattcurf/ollama-intel-gpu
  # https://github.com/eleiton/ollama-intel-arc/blob/main/docker-compose.yml
  ollama-intel:
    profiles: ["intel"]
    <<: *ollama
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    build:
      context: ollama-intel
      dockerfile: Dockerfile
    devices:
      - /dev/dri:/dev/dri
    environment:
      - no_proxy=localhost,127.0.0.1
      - OLLAMA_HOST=0.0.0.0
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1

networks:
  n8n:
    name: n8n
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1
